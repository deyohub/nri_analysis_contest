{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第4回講義 演習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目次\n",
    "1. PyTorch概観\n",
    "2. Tensor  \n",
    "    2.1. Tensorとは  \n",
    "    2.2. Tensorの生成  \n",
    "    2.3. Tensorの操作  \n",
    "    2.4. Tensorの微分  \n",
    "3. Dataset / DataLoader  \n",
    "    3.1. Dataset  \n",
    "    3.2. DataLoader  \n",
    "4. モデルの定義  \n",
    "    4.1. モデルパーツ一覧  \n",
    "    4.2. Sequentialモデル   \n",
    "    4.3. Subclassingモデル  \n",
    "5. モデルの学習  \n",
    "    5.1. 損失関数の設定  \n",
    "    5.2. オプティマイザの設定  \n",
    "    5.3. 学習モード / 推論モード  \n",
    "    5.4. 学習の実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "!pip install torchvision==0.6.1\n",
    "clear_output()\n",
    "\n",
    "import math\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "import torchvision\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "random_seed = 42\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. PyTorch概観\n",
    "\n",
    "PyTorchでは、基本的に以下の流れに沿ってモデルを構築します。\n",
    "\n",
    "1. 学習データの設定\n",
    "2. ネットワークの設定\n",
    "3. 損失関数/オプティマイザの設定\n",
    "4. 学習の実行\n",
    "5. 予測の実行\n",
    "\n",
    "以下に、ORデータに対するロジスティック回帰の例を示します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORデータの作成\n",
    "x_data = np.array([[0, 1], [1, 0], [0, 0], [1, 1]]).astype(np.float32)\n",
    "t_data = np.array([[1], [1], [0], [1]]).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1. 学習データの設定\n",
    "x = torch.tensor(x_data, dtype=torch.float)\n",
    "t = torch.tensor(t_data, dtype=torch.float)\n",
    "\n",
    "# Step 2. ネットワークの設定\n",
    "net = nn.Linear(in_features=2, out_features=1, bias=True)\n",
    "\n",
    "# Step 3. 損失関数の設定\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.1)\n",
    "\n",
    "# Step 4. 学習の実行\n",
    "for epoch in range(1000):\n",
    "    optimizer.zero_grad()  # 勾配の初期化\n",
    "    y = net(x)   #  予測の計算（順伝播）\n",
    "    loss = loss_fn(y, t)  # 損失関数の計算\n",
    "    loss.backward()  # 勾配の計算\n",
    "    optimizer.step()  # パラメータの更新\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print('EPOCH: {}, Train Cost, {:.3f}'.format(epoch + 1, loss))\n",
    "\n",
    "# Step 5. 予測の実行\n",
    "y = net(x)\n",
    "# 正解データの[1,1,0,1]に近い値が得られていることを確認する\n",
    "torch.sigmoid(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tensor\n",
    "\n",
    "### 2.1. Tensorとは\n",
    "PyTorchでは主にTensor型の配列を扱います。先ほどの例ではxやtがそれにあたります。  \n",
    "Tensor型は、NumPyの配列であるndarray型のデータとほぼ同様に扱うことができます。  \n",
    "ndarrayとの相違点は、GPU上の演算が可能であること、勾配情報を保持できることです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[0, 1], [1, 0], [0, 0], [1, 1]],\n",
    "                 dtype=torch.float)\n",
    "\n",
    "print(x)\n",
    "print(type(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データ型には、主に以下の二種類を用います。  \n",
    "- torch.float: float32\n",
    "- torch.long: int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('小数:', torch.float)\n",
    "print('整数:', torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Tensorの生成  \n",
    "Tensorの生成方法には、場合に応じて以下のような分類があります。\n",
    "- torch.tensor() : 既存のデータから生成する場合\n",
    "- torch.* : サイズだけが決まっている場合\n",
    "- torch.*_like : 既存のTensorと同じサイズ・同じデータ型で生成する場合\n",
    "- tensor.new_* : 既存のTensorと異なるサイズ・同じデータ型で生成する場合  \n",
    "\n",
    "ここでは上の二つについて扱います。  \n",
    "先ほどの例では既存のndarrayから、torch.tensor()を用いてTensorを生成しました。  \n",
    "torch.from_numpy()を用いても同様に作成できますが、copyではなくviewとなります。  \n",
    "同様に、既存のリストからtorch.tensor()を用いてTensorを生成することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 既存のndarrayからTensorを生成\n",
    "x_data = np.array([[0, 1], [1, 0], [0, 0], [1, 1]])\n",
    "x = torch.tensor(x_data, dtype=torch.float)\n",
    "print('inputs(Tensor): \\n', x)\n",
    "print()\n",
    "\n",
    "# Tensorからndarrayへの変換\n",
    "x_array = x.numpy()\n",
    "print('inputs(ndarray): \\n', x_array)\n",
    "print()\n",
    "\n",
    "# 既存のリストからTensorを生成\n",
    "t_data = [[1], [1], [0], [1]]\n",
    "t = torch.tensor(t_data, dtype=torch.float)\n",
    "print('outputs(Tensor): \\n', t)\n",
    "print()\n",
    "\n",
    "# Tensorからlistへの変換\n",
    "t_list = t.tolist()\n",
    "print('outputs(list): \\n', t_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所望のサイズだけが決まっている場合は、次のようにtorch.*でTensorを生成することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.Tensor()で初期化されたTensorを生成\n",
    "a = torch.Tensor(2, 3)\n",
    "print('torch.Tensor(): \\n', a)\n",
    "print()\n",
    "\n",
    "# torch.zeros()でゼロ埋めされたTensorを生成\n",
    "b = torch.zeros(3, 4, dtype=torch.int64)\n",
    "print('torch.zeros(): \\n', b)\n",
    "print()\n",
    "\n",
    "# torch.ones()で一埋めされたTensorを生成\n",
    "c = torch.ones(2, 3, 4, dtype=torch.float32)\n",
    "print('torch.ones(): \\n', c)\n",
    "print()\n",
    "\n",
    "# torch.eye()で単位行列のTensorを作成\n",
    "d = torch.eye(3)\n",
    "print('torch.eye(): \\n', d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "乱数で初期化されたTensorは、以下のように生成することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.rand()で一様乱数からTensorを生成\n",
    "e = torch.rand(2, 3)\n",
    "print('torch.rand(): \\n', e)\n",
    "print()\n",
    "\n",
    "# torch.randn()で標準正規乱数からTensorを生成\n",
    "f = torch.randn(2, 3)\n",
    "print('torch.randn(): \\n', f)\n",
    "print()\n",
    "\n",
    "# torch.randperm()でランダムな順列からTensorを生成\n",
    "g = torch.randperm(10)\n",
    "print('torch.randperm(): \\n', g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "numpy-likeに数列を生成することも可能です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.arange()で、numpy.arange()と同様にTensorを生成\n",
    "h = torch.arange(0, 1, 0.2)\n",
    "print('torch.arange(): \\n', h)\n",
    "print()\n",
    "\n",
    "# torch.linspace()で、numpy.linspace()と同様にTensorを生成\n",
    "i = torch.linspace(0, 10, 5)\n",
    "print('torch.linspace(): \\n', i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Tensorの操作\n",
    "GPU上で計算を行う際には、TensorをGPUに乗せる必要があります。  \n",
    "CPUとGPUの切り替えには、以下のようにtoメソッドを用います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[0, 1], [1, 0], [0, 0], [1, 1]],\n",
    "                 dtype=torch.float)\n",
    "\n",
    "# tensor.deviceで、device(CPUかGPUか)の確認\n",
    "x = x.to('cpu')\n",
    "print('現在のdevice:', x.device)\n",
    "print()\n",
    "\n",
    "# tensor.to('cuda:0')で、GPUに切り替え\n",
    "x = x.to('cuda:0')\n",
    "print('GPUに切替:', x.device)\n",
    "\n",
    "# tensor.to('cpu')で、CPUに切り替え\n",
    "x = x.to('cpu')\n",
    "print('CPUに切替:', x.device)\n",
    "print()\n",
    "\n",
    "# torch.cuda.is_available()で、GPUが利用可能か確認\n",
    "print('GPUが利用可能かどうかチェック:', torch.cuda.is_available())\n",
    "\n",
    "# GPUが利用可能ならGPUに切り替え、そうでないならCPUに切り替え\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "x = x.to(device)\n",
    "print('GPUが利用可能ならGPUに切替:', x.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorはndarrayと同様にスライスすることができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[0, 1], [1, 0], [0, 0], [1, 1]],\n",
    "                 dtype=torch.float)\n",
    "\n",
    "print('スライス前のTensor: \\n', x)\n",
    "print()\n",
    "\n",
    "print('2列目を切り出し: \\n', x[:, 1])\n",
    "print()\n",
    "\n",
    "print('最終行を切り出し: \\n', x[-1, :])\n",
    "print()\n",
    "\n",
    "print('2行目までを切り出し: \\n', x[:2, :])\n",
    "print()\n",
    "\n",
    "print('1行おきに切り出し: \\n', x[::2, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorの情報は、以下のように取得できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[0, 1], [1, 0], [0, 0], [1, 1]],\n",
    "                 dtype=torch.float)\n",
    "\n",
    "# tensor.deviceでdeviceを取得\n",
    "print('tensor.device: ', x.device)\n",
    "\n",
    "# tensor.dtypeでデータ型を取得\n",
    "print('tensor.dtype : ', x.dtype)\n",
    "\n",
    "# tensor.shapeで形状を取得（tensor.size()でも同様）\n",
    "print('tensor.shape : ', x.shape)\n",
    "\n",
    "# tensor.dim()で次元を取得\n",
    "print('tensor.dim() : ', x.dim())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorの結合は、以下のように実行できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = torch.zeros(2, 3)\n",
    "x2 = torch.ones(2, 3)\n",
    "\n",
    "# torch.cat()で既存の軸方向に結合（元データと次元は変わらない）\n",
    "x_cat = torch.cat([x1, x2], dim=0)\n",
    "print('縦方向に結合: \\n', x_cat)\n",
    "print('Tensorの次元: ', x_cat.dim())\n",
    "print('Tensorのサイズ:', x_cat.size(), '\\n')\n",
    "\n",
    "\n",
    "# torch.stack()で新規の軸方向に結合（元データより次元が大きくなる）\n",
    "x_stack = torch.stack([x1, x2], dim=0)\n",
    "print('新規の軸方向に結合: \\n', x_stack)\n",
    "print('Tensorの次元：', x_stack.dim())\n",
    "print('Tensorのサイズ', x_stack.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorの変形は、以下のように実行できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x3 = torch.arange(0, 12)\n",
    "print('変形前のTensor: \\n', x3)\n",
    "print()\n",
    "\n",
    "# tensor.reshape()で、指定した形状に変形\n",
    "x3 = x3.reshape(1, 3, 4)\n",
    "print('tensor.reshape(): \\n', x3)\n",
    "print()\n",
    "\n",
    "# tensor.transpose()で指定した軸を交換\n",
    "x3 = x3.transpose(1, 2)\n",
    "print('tensor.transpose(): \\n', x3)\n",
    "print()\n",
    "\n",
    "# tensor.squeeze()で要素数が1の次元を削除\n",
    "x3 = x3.squeeze()\n",
    "print('tensor.squeeze(): \\n', x3)\n",
    "print()\n",
    "\n",
    "# tensor.unsqueeze()で指定した次元を追加\n",
    "x3 = x3.unsqueeze(1)\n",
    "print('tensor.unsqueeze(): \\n', x3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorの分割は、以下のように実行できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x4 = torch.arange(0, 12)\n",
    "x4 = x4.reshape(3, 4)\n",
    "print('分割前のTensor: \\n', x4)\n",
    "print()\n",
    "\n",
    "# torch.chunk()で指定した個数の塊に分割\n",
    "x_chunks = torch.chunk(x4, 2, dim=0)\n",
    "print('torch.chunk(): \\n', x_chunks)\n",
    "print()\n",
    "\n",
    "# torch.split()で指定した個数ごとに分割\n",
    "x_splits = torch.split(x4, 2, dim=1)\n",
    "print('torch.splits(): ')\n",
    "pprint(x_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Tensorの微分\n",
    "深層学習での計算、特に誤差逆伝播法では計算グラフという概念が用いられます。  \n",
    "計算グラフとは、複数の関数からなる計算過程をグラフとして表したものです。  \n",
    "計算グラフでは、各演算ノード間の勾配が計算され、保持される必要があります。  \n",
    "PyTorchでは、以下のようにTensor型がこの勾配の計算と保持を実現できます。  \n",
    "内部的には主にautogradパッケージによって自動微分機能が提供されています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一次関数 y=wx+b を例に自動微分を実行\n",
    "\n",
    "# requires_grad=Trueとして自動微分の対象を指定\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "w = torch.tensor(3.0, requires_grad=True)\n",
    "b = torch.tensor(4.0, requires_grad=True)\n",
    "\n",
    "# 計算グラフを構築\n",
    "y = w*x + b\n",
    "\n",
    "# tensor.backward()で勾配を計算\n",
    "# 計算グラフを遡って、requires_grad=TrueとしたTensorに勾配の情報を付与\n",
    "y.backward()\n",
    "\n",
    "# tensor.gradでその変数による偏微分値を表示\n",
    "print('dy/dx(expected to be 3): ', x.grad)  # dy/dx = w = 3.\n",
    "print('dy/dw(expected to be 2): ', w.grad)  # dy/dw = x = 2.\n",
    "print('dy/db(expected to be 1): ', b.grad)  # dy/db = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "深層学習では、損失関数を重みやバイアスで偏微分した値が主に用いられます。  \n",
    "初めに見たロジスティック回帰の例では、BCEに対し勾配を計算していました。  \n",
    "ここではナイーブな勾配降下法を例にとって、最適化の挙動を見てみましょう。\n",
    "\n",
    "- optim.SGD : (最適化する係数，その他オプション)を入力として与える．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1. 学習データの設定\n",
    "x = torch.randn(5, 3)\n",
    "y = torch.randn(5, 2)\n",
    "\n",
    "# Step 2. ネットワークの設定\n",
    "linear = nn.Linear(3, 2)\n",
    "\n",
    "# Step 3. 損失関数・オプティマイザの設定\n",
    "criterion = nn.MSELoss() # 平均二乗誤差を損失関数として採用\n",
    "optimizer = torch.optim.SGD(linear.parameters(), lr=0.01) # 確率的勾配降下法（オンライン学習）で学習\n",
    "\n",
    "# Step 4. 学習の実行(1 epoch)\n",
    "optimizer.zero_grad()\n",
    "pred = linear(x)  # 予測の計算（順伝播）\n",
    "loss = criterion(pred, y)  # 損失関数の計算\n",
    "loss.backward()  # 勾配の計算（逆伝播）\n",
    "print('損失関数の微分値:')\n",
    "print('dL/dw:', linear.weight.grad)  # 重みに関する偏微分\n",
    "print('dL/db:', linear.bias.grad)  # バイアスに関する偏微分\n",
    "print()\n",
    "\n",
    "print('手書きの最急降下法:')  # 手書きの最急降下法\n",
    "print(linear.weight.sub(0.01 * linear.weight.grad))\n",
    "print()\n",
    "print(linear.bias.sub(0.01 * linear.bias.grad))\n",
    "print()\n",
    "\n",
    "print('実装済みの最急降下法:')  # 実装済みの最急降下法\n",
    "optimizer.step()\n",
    "print(linear.weight)\n",
    "print(linear.bias)  # 結果が一致することを確認"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset / DataLoader\n",
    "深層学習における入力データの扱いには、以下のようなステップがあります。\n",
    "1. 読み込み\n",
    "2. 前処理\n",
    "3. ラベル付け\n",
    "4. シャッフル\n",
    "4. 分割\n",
    "\n",
    "PyTorchでは1から3をDatasetが、4と5をDataLoaderがそれぞれ実現します。\n",
    "\n",
    "### 3.1. Dataset\n",
    "Datasetは与えられたデータを読み込んで前処理を施し、ラベルを付与します。  \n",
    "ここでは後でも取り上げるMNISTデータを例に、その扱いをみてみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPUの利用可否を取得\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# MNISTのデータセットを取得\n",
    "mnist = fetch_openml('mnist_784', data_home='/root/userspace/public/day1/chap02/data')\n",
    "X = mnist.data.astype('float32')\n",
    "y = mnist.target.astype('int64')\n",
    "\n",
    "# 学習データの値域を[0,1]に正規化\n",
    "X /= 255\n",
    "\n",
    "# 訓練データとテストデータに分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/7, shuffle=False)\n",
    "\n",
    "# データを変形（画像数， チャネル数， 高さ， 幅）\n",
    "X_train = X_train.reshape(60000, 1, 28, 28)\n",
    "X_test = X_test.reshape(10000, 1, 28, 28)\n",
    "\n",
    "# Tensorに変換\n",
    "X_train = torch.tensor(X_train, device=device, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, device=device, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, device=device, dtype=torch.int64)\n",
    "y_test = torch.tensor(y_test, device=device, dtype=torch.int64)\n",
    "\n",
    "# データセットを作成\n",
    "dataset_train = TensorDataset(X_train, y_train)\n",
    "dataset_test = TensorDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.utils.data.Datasetを継承することで、Datasetを自作することもできます。  \n",
    "Datasetを自作する際には、\\_\\_len\\_\\_と\\_\\_getitem\\_\\_を定義する必要があります。  \n",
    "- \\_\\_len\\_\\_: データセットの長さを返すメソッド  \n",
    "- \\_\\_getitem\\_\\_: 説明変数と目的変数の組を一つ返す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.utils.data.Datasetを継承し、自作データセットを作成\n",
    "class MNISTDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, transform=None):\n",
    "        # MNISTデータのダウンロード\n",
    "        self.mnist = fetch_openml('mnist_784', data_home='/root/userspace/public/day1/chap02/data')\n",
    "        self.data = self.mnist.data.reshape(-1, 28, 28).astype('uint8')  # 説明変数\n",
    "        self.target = self.mnist.target.astype('int64')  # 目的変数\n",
    "        self.transform = transform  # 前処理\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)  # データセットの長さを返す\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = self.data[idx]\n",
    "        target = torch.from_numpy(np.array(self.target[idx]))\n",
    "        \n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "                \n",
    "        sample = (data, target)\n",
    "        return sample  # 説明変数と目的変数の組を一つ返す"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNISTのような有名なデータセットは、予め用意されているものを利用できます。  \n",
    "この場合、torchvision.transformsパッケージを以下のように用いて前処理します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画像データの前処理を定義\n",
    "trans = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Tensorに変換\n",
    "    transforms.Normalize((0.5, ), (0.5, ))  # 平均0.5, 標準偏差0.5に正規化\n",
    "])\n",
    "\n",
    "# ダウンロードをしてから，mnistディレクトリに展開\n",
    "mnist_train = MNIST('/root/userspace/public/day1/chap04/data/mnist', train=True, download=True, transform=trans)\n",
    "mnist_test = MNIST('/root/userspace/public/day1/chap04/data/mnist', train=False, download=True, transform=trans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasetは以下のようにtorch.utils.data.random_split()を用いて分割できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.utils.data.random_split()によって訓練データと検証データに分割\n",
    "mnist_train, mnist_valid = torch.utils.data.random_split(mnist_train, [48000, 12000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. DataLoader\n",
    "DataLoaderを用いることで、Datasetをシャッフルし、ミニバッチに分割することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練データのDataLoaderを作成\n",
    "loader_train = DataLoader(mnist_train, \n",
    "                          shuffle=True,  # データをシャッフルする\n",
    "                          batch_size=1024,  # ミニバッチの大きさ: 1024(＊ただし最後の１つはあまり)\n",
    "                          num_workers=4  # コア数: 4\n",
    "                         )\n",
    "\n",
    "# 検証データのDataLoaderを作成\n",
    "loader_valid = DataLoader(mnist_valid,\n",
    "                          shuffle=True,  # データをシャッフルする\n",
    "                          batch_size=1024,  # ミニバッチの大きさ: 1024\n",
    "                          num_workers=4  # コア数: 4\n",
    "                         )\n",
    "\n",
    "# テストデータのDataLoaderを作成\n",
    "loader_test = DataLoader(mnist_test,\n",
    "                         shuffle=True,  # データをシャッフルする\n",
    "                         batch_size=1024,  # ミニバッチの大きさ: 1024\n",
    "                         num_workers=4  # コア数: 4\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "作成したDataLoaderからは、次のようにしてミニバッチごとにデータを取り出すことができます。\n",
    "\n",
    "\n",
    "`shuffle=True` と指定したため，毎回違う順番になります．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in loader_train:\n",
    "    inputs, labels = data\n",
    "    break\n",
    "print('ミニバッチの大きさ: ', len(labels))\n",
    "print('ミニバッチの中身:', labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. モデル定義\n",
    "PyTorchにおけるモデル定義には、以下の二つの方法があります。  \n",
    "- Sequentialモデル\n",
    "- Subclassingモデル\n",
    "\n",
    "まずはPyTorchにおいてよく用いられる深層学習モデルのパーツを見てみます。  \n",
    "\n",
    "\n",
    "### 4.1. モデルパーツ一覧\n",
    "\n",
    "PyTorchにおける深層学習モデルのパーツには、以下のようなものがあります。  \n",
    "（以下に挙げる例の中には、まだ講義で扱っていない内容も含まれます。）\n",
    "- 全結合層 : torch.nn.Linear\n",
    "- ドロップアウト : torch.nn.Dropout\n",
    "- 畳み込み層(2次元) : torch.nn.Conv2d\n",
    "- 最大プーリング(2次元) : torch.nn.MaxPool2d\n",
    "- バッチ正規化(2次元) : torch.nn.BatchNorm2d\n",
    "- RNN層 : torch.nn.RNN\n",
    "- LSTM層 : torch.nn.LSTM\n",
    "- シグモイド関数 : torch.nn.Sigmoid\n",
    "- tanh関数 : torch.nn.Tanh\n",
    "- ReLU関数 : torch.nn.ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全結合層\n",
    "m = nn.Linear(20, 30)\n",
    "input = torch.randn(128, 20)\n",
    "output = m(input) \n",
    "print('全結合層の出力の大きさ:', output.shape) # (128 * 20) * (20 * 30)なので．．．\n",
    "print()\n",
    "\n",
    "# ドロップアウト\n",
    "m = nn.Dropout(p=0.4) # 40%の値を0にする．\n",
    "input = torch.randn(3, 5)\n",
    "output = m(input)\n",
    "print('ドロップアウト層の出力: \\n', output)\n",
    "print()\n",
    "\n",
    "# ReLU関数\n",
    "m = nn.ReLU()\n",
    "input = torch.tensor([1.2, -0.8])\n",
    "output = m(input)\n",
    "print('ReLU関数の入力: ', input)\n",
    "print('ReLU関数の出力: ', output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Sequentialモデル\n",
    "torch.nn.Sequentialクラスを用いるSequentialモデルは、以下のようにして各層を順々に積み重ねていくものです。  \n",
    "この記法はKerasのSequentialモデルAPIに準じています。直感的な記述が可能ですが、単純なモデルしか組めません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### モデルのインスタンスを作成\n",
    "model = nn.Sequential()\n",
    "\n",
    "### add_module()で層を追加\n",
    "model.add_module('fc1', nn.Linear(2, 5))\n",
    "model.add_module('sigmoid', nn.Sigmoid())\n",
    "model.add_module('fc2', nn.Linear(5, 1))\n",
    "\n",
    "### 作成したモデルの情報を表示\n",
    "summary(model.to(device), (1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Subclassingモデル\n",
    "もう一つは、torch.nn.Moduleクラスのサブクラスとして定義する記法です。  \n",
    "本講座では、より汎用性の高いこちらの記法をメインとして採用します。  \n",
    "Subclassingモデルにも、以下のように大きく分けて二種類の記法があります。\n",
    "- パラメタを持つものは\\_\\_init\\_\\_に、持たないものはforwardに書く\n",
    "- パラメタを持たない活性化関数なども含めて全て\\_\\_init\\_\\_に書く  \n",
    "\n",
    "これら二つにもそれぞれ長所と短所があり、使い分けられるのが望ましいです。  \n",
    "まず前者の実装例を見てみましょう。利点は以下のようなものが挙げられます。\n",
    "- 各層のパラメタの有無が明確になる\n",
    "- モデルを簡潔に記述することができる  \n",
    "\n",
    "この場合、活性化関数はtorch.nn.functional(as F)のものを使います。  \n",
    "例えばReLU関数はnn.ReLUではなく、F.reluをforward内に記述します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.Moduleクラスを継承してクラス定義\n",
    "# パラメタを持つものは__init__に、持たないものはforwardに書く\n",
    "class Model1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model1, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 5)\n",
    "        self.fc2 = nn.Linear(5, 1)\n",
    "        \n",
    "    # 順伝播の定義\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# モデルのインスタンスを作成\n",
    "model1 = Model1()\n",
    "\n",
    "# 作成したモデルの情報を表示\n",
    "summary(model1.to(device), (1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に後者の実装例を見てみましょう。利点は、以下のようなものが挙げられます。\n",
    "- printやsummaryで活性化関数なども表示できる\n",
    "- 学習/推論モードがdropoutなどに自動で反映される"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.Moduleクラスを継承してクラス定義\n",
    "# パラメタを持たない活性化関数なども含めて全て__init__に書く\n",
    "class Model2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model2, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 5)\n",
    "        self.ac1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(5, 1)\n",
    "        \n",
    "    # 順伝播の定義\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.ac1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# モデルのインスタンスを作成\n",
    "model2 = Model2()\n",
    "\n",
    "# 作成したモデルの情報を表示\n",
    "summary(model2.to(device), (1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.Sequentialの使い方の例\n",
    "深層学習の火付け役となったAlexNetの実装を見てみましょう。このようにいくつかの記法を組み合わせることも可能です。  \n",
    "この実装例のようにいくつかの記法を組み合わせて記述することで、よりコンパクトで可読性の高いモデル定義ができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super().__init__()\n",
    "        # 畳み込みブロックのSequentialによる定義\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
    "        # 全結合ブロックのSequentialによる定義\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "alexnet = AlexNet()\n",
    "\n",
    "summary(alexnet.to(device), (3, 224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 学習\n",
    "### 5.1. 損失関数の設定\n",
    "PyTorchでは、torch.nnパッケージで主要な損失関数が実装されているので、これを用います。  \n",
    "- 回帰\n",
    "    - nn.MSELoss(平均二乗誤差)\n",
    "    - nn.L1Loss(平均絶対誤差)\n",
    "- 2クラス分類\n",
    "    - nn.BCELoss(2クラス交差エントロピー)\n",
    "    - nn.BCEWithLogitsLoss(ロジット2クラス交差エントロピー)\n",
    "- 多クラス分類\n",
    "    - nn.CrossEntropyLoss(多クラス交差エントロピー)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn(1000)\n",
    "target = torch.randn(1000)\n",
    "\n",
    "mse_fn = nn.MSELoss()\n",
    "mse = mse_fn(input, target)\n",
    "print('平均二乗誤差:', mse)\n",
    "\n",
    "\n",
    "input = torch.randn(3, 5)\n",
    "target = torch.randperm(3, dtype=torch.long)\n",
    "\n",
    "cel_fn = nn.CrossEntropyLoss()\n",
    "cel = cel_fn(input, target)\n",
    "print('交差エントロピー誤差:', cel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. オプティマイザの設定\n",
    "PyTorchでは、torch.optimパッケージ内に主要なオプティマイザが実装されています。\n",
    "- torch.optim.SGD\n",
    "- torch.optim.Adagrad\n",
    "- torch.optim.Adadelta\n",
    "- torch.optim.Adam\n",
    "\n",
    "「2.4. Tensorの微分」で見たナイーブな最急降下法の例を、改めて見ておきましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1. 学習データの設定\n",
    "x = torch.randn(5, 3)\n",
    "y = torch.randn(5, 2)\n",
    "\n",
    "# Step 2. ネットワークの設定\n",
    "linear = nn.Linear(3, 2)\n",
    "\n",
    "# Step 3. 損失関数・オプティマイザの設定\n",
    "criterion = nn.MSELoss()\n",
    "rate = 0.01\n",
    "optimizer = torch.optim.SGD(linear.parameters(), lr=rate)\n",
    "\n",
    "# Step 4. 学習の実行(1 epoch)\n",
    "pred = linear(x)  # 予測の計算（順伝播）\n",
    "loss = criterion(pred, y)  # 損失関数の計算\n",
    "loss.backward()  # 勾配の計算（逆伝播）\n",
    "print('損失関数の微分値:')\n",
    "print('dL/dw:', linear.weight.grad)  # 重みに関する偏微分\n",
    "print('dL/db:', linear.bias.grad)  # バイアスに関する偏微分\n",
    "print('')\n",
    "\n",
    "print('手書きの最急降下法: ')  # 手書きの最急降下法\n",
    "print(linear.weight.sub(rate * linear.weight.grad))\n",
    "print(linear.bias.sub(rate *linear.bias.grad))\n",
    "print('')\n",
    "\n",
    "print('実装済みの最急勾配法: ')  # 実装済みの最急勾配法\n",
    "optimizer.step()\n",
    "print(linear.weight)\n",
    "print(linear.bias)  # 結果が一致することを確認"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 学習モード / 推論モード\n",
    "　PyTorchでは、以下の二つのモードがあります。\n",
    "- 学習モード\n",
    "- 推論モード\n",
    "\n",
    "学習モードと推論モードの大きな違いはパラメータ更新の有無です。  \n",
    "学習モードではパラメータが更新されますが、推論モードではパラメータが更新されません。\n",
    "\n",
    "また学習モードと推論モードで、ドロップアウト(torch.nn.Dropout)の挙動も変わります。  \n",
    "学習モードではドロップアウトが有効ですが、推論モードでは自動で無効となります。  \n",
    "ただしtorch.nn.functional.dropoutでは自動で切り替わらないので、注意する必要があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルを作成\n",
    "model = nn.Sequential()\n",
    "\n",
    "# 学習モードか推論モードか確認(model.trainingがTrueなら学習モード、Falseなら推論モード)\n",
    "print('初期モード: ', model.training)\n",
    "\n",
    "# 推論モードに切替\n",
    "model.eval()\n",
    "print('推論モードに切替後: ', model.training)\n",
    "\n",
    "# 学習モードに切替\n",
    "model.train()\n",
    "print('学習モードに切替後: ', model.training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "推論段階ではパラメータを更新しないため、計算グラフを構築する必要がありません。  \n",
    "計算グラフを構築しないよう設定することによって、メモリを節約することができます。  \n",
    "この設定は、with torch.no_grad()内で対象となる処理を記述することで実現できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(3.0, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "# torch.no_grad()との比較用\n",
    "y = x ** 2\n",
    "y.backward()\n",
    "print('dy/dx(expected to be 6.0): ', x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(3.0, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "# 計算グラブを構築しないため、backward()でエラーが発生する\n",
    "with torch.no_grad():\n",
    "    y = x ** 2\n",
    "    y.backward()\n",
    "    x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4. 学習の実行\n",
    "これまでに扱った内容を駆使して、MNISTデータのMLPによる分類を実装してみます。\n",
    "1. 学習データの設定\n",
    "2. ネットワークの設定\n",
    "3. 損失関数/オプティマイザの設定\n",
    "4. 学習の実行\n",
    "5. 予測の実行\n",
    "\n",
    "以下では中間層が一層だけのMLPをバッチサイズ1024、エポック数10で学習します。\n",
    "\n",
    "＜備考＞\n",
    "- Net classはnn.Moduleを継承するclass．したがって，`python super().__init__()`によってnn.Moduleもinitする必要がある．\n",
    "- 精度向上のために，ハイパーパラメーターを調節してみると良い．\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1. 学習データの設定\n",
    "# 前処理の設定\n",
    "trans = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Tensorに変換\n",
    "    transforms.Normalize((0.5, ), (0.5, ))  # 平均0.5, 標準偏差0.5に正規化\n",
    "])\n",
    "\n",
    "# Datasetの作成\n",
    "ds_train = MNIST('/root/userspace/public/day1/chap04/data/mnist', train=True, download=True, transform=trans)\n",
    "ds_test = MNIST('/root/userspace/public/day1/chap04/data/mnist', train=False, download=True, transform=trans)\n",
    "ds_train, ds_valid = torch.utils.data.random_split(ds_train, [48000, 12000])\n",
    "\n",
    "# DataLoaderの作成\n",
    "batch_size = 1024  # バッチサイズ: 1024\n",
    "dl_train = DataLoader(ds_train, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "dl_valid = DataLoader(ds_valid, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "dl_test = DataLoader(ds_test, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "\n",
    "# Step 2. ネットワークの設定\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(28 * 28, 200)  # 入力層\n",
    "        self.l2 = nn.Linear(200, 200)  # 中間層\n",
    "        self.l3 = nn.Linear(200, 10)  # 出力層\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.reshape(-1, 28 * 28)  # 二次元の画像データを一次元のベクトルに変形\n",
    "        x = F.relu(self.l1(x))  # 活性化関数は全てReLU\n",
    "        x = F.relu(self.l2(x))\n",
    "        x = self.l3(x)\n",
    "        return x\n",
    "    \n",
    "# GPUが利用可能ならGPUを使用\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "net = Net().to(device)\n",
    "\n",
    "\n",
    "# Step 3. 損失関数/オプティマイザの設定\n",
    "loss_fn = nn.CrossEntropyLoss()  # 損失関数: 交差エントロピー誤差\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.1)  # オプティマイザ: 最急降下法\n",
    "\n",
    "\n",
    "# Step 4. 学習の実行\n",
    "num_epochs = 10  # エポック数: 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc, val_loss, val_acc = 0.0, 0.0, 0.0, 0.0\n",
    "    \n",
    "    net.train()  # 学習モードに切替\n",
    "    for inputs, labels in dl_train:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)  # データをGPUに乗せる\n",
    "        optimizer.zero_grad()  # 勾配の初期化\n",
    "        outputs = net(inputs)  # 予測の計算（順伝播）\n",
    "        loss = loss_fn(outputs, labels)  # 損失関数の計算\n",
    "        loss.backward()  # 勾配の計算(逆伝播)\n",
    "        train_loss += loss.item()  # 損失の加算\n",
    "        acc = (outputs.max(1)[1] == labels).sum() # 正答数の数え上げ\n",
    "        train_acc += acc.item()  # 正答数の加算\n",
    "        optimizer.step()  # パラメータの更新\n",
    "    avg_train_loss = train_loss / len(dl_train.dataset)  # 平均損失の計算\n",
    "    avg_train_acc = train_acc / len(dl_train.dataset)  # 正答率の計算\n",
    "        \n",
    "    net.eval()  # 推論モードに切替\n",
    "    with torch.no_grad():  # 計算グラフの構築をしないよう設定\n",
    "        for inputs, labels in dl_valid:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)  # データをGPUに乗せる\n",
    "            outputs = net(inputs)  # 予測の計算(順伝播)\n",
    "            loss = loss_fn(outputs, labels)  # 損失関数の計算\n",
    "            val_loss += loss.item()  # 損失の加算\n",
    "            acc = (outputs.max(1)[1] == labels).sum()  # 正答数の数え上げ\n",
    "            val_acc += acc.item()  # 正答数の加算\n",
    "    avg_val_loss = val_loss / len(dl_valid.dataset)  # 平均損失の計算\n",
    "    avg_val_acc = val_acc / len(dl_valid.dataset)  # 正答率の計算\n",
    "    \n",
    "    print(('Epoch [{}/{}], train_loss: {train_loss:.5f}, train_acc: {train_acc:.3f}, '\n",
    "          'val_loss: {val_loss:.5f}, val_acc: {val_acc:.3f}')\n",
    "          .format(epoch+1, num_epochs, train_loss=avg_train_loss, \n",
    "                  train_acc=avg_train_acc, val_loss=avg_val_loss, val_acc=avg_val_acc))\n",
    "\n",
    "    \n",
    "# Step 5. 予測の実行\n",
    "net.eval()  # 推論モードに切替\n",
    "with torch.no_grad():  # 計算グラフの構築をしないよう設定\n",
    "    total = 0\n",
    "    test_acc = 0\n",
    "    for inputs, labels in dl_test:        \n",
    "        inputs, labels = inputs.to(device), labels.to(device)  # データをGPUに乗せる\n",
    "        outputs = net(inputs)  # 予測の計算\n",
    "        test_acc += (outputs.max(1)[1] == labels).sum().item()  # 正解数の数え上げと加算\n",
    "        total += labels.size(0)  # テストデータの大きさを取得\n",
    "    print('Test Accuracy: {} %'.format(100 * test_acc / total)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
